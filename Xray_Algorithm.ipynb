{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea5bd234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e158d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_yolo_dataset(dataset_path, output_path=\"xray_yolo_dataset\", test_split=0.2):\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ“¦ PREPARING YOLO DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    dataset_path = Path(dataset_path)\n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    # Verify dataset path exists\n",
    "    if not dataset_path.exists():\n",
    "        raise FileNotFoundError(f\"âŒ Dataset path does not exist: {dataset_path}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ Dataset path: {dataset_path.absolute()}\")\n",
    "    \n",
    "    # Create YOLO directory structure\n",
    "    (output_path / \"images\" / \"train\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_path / \"images\" / \"val\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_path / \"labels\" / \"train\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_path / \"labels\" / \"val\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get class names from folder structure\n",
    "    class_names = sorted([d.name for d in dataset_path.iterdir() if d.is_dir()])\n",
    "    \n",
    "    if not class_names:\n",
    "        raise ValueError(f\"âŒ No class folders found in {dataset_path}!\\n\"\n",
    "                        \"Make sure your dataset has subfolders for each class.\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Classes found: {class_names}\")\n",
    "    \n",
    "    class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "    \n",
    "    # Process each class\n",
    "    all_images = []\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_folder = dataset_path / class_name\n",
    "        class_idx = class_to_idx[class_name]\n",
    "        \n",
    "        # Get all images (handle both uppercase and lowercase extensions)\n",
    "        image_files = (\n",
    "            list(class_folder.glob(\"*.jpg\")) +\n",
    "            list(class_folder.glob(\"*.JPG\")) +\n",
    "            list(class_folder.glob(\"*.png\")) +\n",
    "            list(class_folder.glob(\"*.PNG\")) +\n",
    "            list(class_folder.glob(\"*.jpeg\")) +\n",
    "            list(class_folder.glob(\"*.JPEG\"))\n",
    "        )\n",
    "        \n",
    "        print(f\"   {class_name}: {len(image_files)} images\")\n",
    "        \n",
    "        if len(image_files) == 0:\n",
    "            print(f\"      âš ï¸  WARNING: No images found in {class_name}/ folder!\")\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            all_images.append((img_file, class_idx, class_name))\n",
    "    \n",
    "    # Check if we found any images\n",
    "    if len(all_images) == 0:\n",
    "        raise ValueError(\n",
    "            f\"âŒ No images found in dataset!\\n\\n\"\n",
    "            f\"Checked path: {dataset_path.absolute()}\\n\"\n",
    "            f\"Found folders: {class_names}\\n\\n\"\n",
    "            f\"Make sure:\\n\"\n",
    "            f\"  1. Images are inside the class folders\\n\"\n",
    "            f\"  2. Images have extensions: .jpg, .JPG, .png, .PNG, .jpeg, .JPEG\\n\"\n",
    "            f\"  3. Path is correct\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nâœ… Total images found: {len(all_images)}\")\n",
    "    \n",
    "    # Split into train and validation\n",
    "    train_data, val_data = train_test_split(\n",
    "        all_images, \n",
    "        test_size=test_split, \n",
    "        random_state=42,\n",
    "        stratify=[x[1] for x in all_images]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Split complete:\")\n",
    "    print(f\"   Training: {len(train_data)} images\")\n",
    "    print(f\"   Validation: {len(val_data)} images\")\n",
    "    \n",
    "    # Copy files and create labels\n",
    "    print(\"\\nğŸ“ Copying files and creating labels...\")\n",
    "    \n",
    "    def process_split(data, split_name):\n",
    "        for img_path, class_idx, class_name in data:\n",
    "            # Copy image\n",
    "            new_img_name = f\"{class_name}_{img_path.stem}.jpg\"\n",
    "            new_img_path = output_path / \"images\" / split_name / new_img_name\n",
    "            shutil.copy(img_path, new_img_path)\n",
    "            \n",
    "            # Create label file (YOLO format: class x_center y_center width height)\n",
    "            # For classification, we assume the entire image is the object\n",
    "            label_path = output_path / \"labels\" / split_name / f\"{new_img_name.replace('.jpg', '.txt')}\"\n",
    "            \n",
    "            # YOLO format: normalized coordinates\n",
    "            # For full image classification: center at 0.5, 0.5, full width/height = 1.0\n",
    "            with open(label_path, 'w') as f:\n",
    "                f.write(f\"{class_idx} 0.5 0.5 1.0 1.0\\n\")\n",
    "    \n",
    "    process_split(train_data, \"train\")\n",
    "    process_split(val_data, \"val\")\n",
    "    \n",
    "    print(\"âœ… Files copied and labels created!\")\n",
    "    \n",
    "    # Create data.yaml file\n",
    "    data_yaml = {\n",
    "        'path': str(output_path.absolute()),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'nc': len(class_names),  # number of classes\n",
    "        'names': class_names\n",
    "    }\n",
    "    \n",
    "    yaml_path = output_path / \"data.yaml\"\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(data_yaml, f)\n",
    "    \n",
    "    print(f\"\\nâœ… data.yaml created at: {yaml_path}\")\n",
    "    \n",
    "    return class_names, str(yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e525bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# STEP 2: MODEL TRAINING\n",
    "# ============================================\n",
    "\n",
    "def train_yolo_model(data_yaml_path, epochs=50, img_size=640, batch_size=16, model_name=\"yolov8n.pt\"):    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸš€ STARTING YOLO TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Training Configuration:\")\n",
    "    print(f\"   Model: {model_name}\")\n",
    "    print(f\"   Epochs: {epochs}\")\n",
    "    print(f\"   Image Size: {img_size}\")\n",
    "    print(f\"   Batch Size: {batch_size}\")\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    print(f\"\\nğŸ“¦ Loading pre-trained {model_name}...\")\n",
    "    model = YOLO(model_name)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nğŸ‹ï¸ Training started...\\n\")\n",
    "    \n",
    "    results = model.train(\n",
    "        data=data_yaml_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        patience=10,  # Early stopping patience\n",
    "        save=True,\n",
    "        device='cpu',  # Change to 'cuda' or 'mps' if you have GPU\n",
    "        workers=2,\n",
    "        project='xray_runs',\n",
    "        name='xray_detector',\n",
    "        exist_ok=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… TRAINING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d54d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 3: MODEL EVALUATION\n",
    "# ============================================\n",
    "\n",
    "def evaluate_model(model, data_yaml_path, img_size=640):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š EVALUATING MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Validate the model\n",
    "    metrics = model.val(\n",
    "        data=data_yaml_path,\n",
    "        imgsz=img_size,\n",
    "        batch=16,\n",
    "        device='cpu',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nğŸ“ˆ Validation Results:\")\n",
    "    print(f\"   Precision: {metrics.box.mp:.4f}\")\n",
    "    print(f\"   Recall: {metrics.box.mr:.4f}\")\n",
    "    print(f\"   mAP@50: {metrics.box.map50:.4f}\")\n",
    "    print(f\"   mAP@50-95: {metrics.box.map:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b384b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 4: TEST ON SAMPLE IMAGES\n",
    "# ============================================\n",
    "\n",
    "def test_on_images(model, test_images_path, class_names, confidence_threshold=0.5):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ§ª TESTING ON SAMPLE IMAGES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_path = Path(test_images_path)\n",
    "    image_files = list(test_path.glob(\"*.jpg\")) + \\\n",
    "                 list(test_path.glob(\"*.png\")) + \\\n",
    "                 list(test_path.glob(\"*.jpeg\"))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"âŒ No test images found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ“¸ Found {len(image_files)} test images\")\n",
    "    \n",
    "    # Test on each image\n",
    "    for i, img_path in enumerate(image_files[:10], 1):  # Test first 10 images\n",
    "        print(f\"\\n{i}. Testing: {img_path.name}\")\n",
    "        \n",
    "        # Run inference\n",
    "        results = model(str(img_path), conf=confidence_threshold, verbose=False)\n",
    "        \n",
    "        # Get predictions\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            \n",
    "            if len(boxes) > 0:\n",
    "                # Get the most confident detection\n",
    "                conf = boxes.conf[0].item()\n",
    "                cls = int(boxes.cls[0].item())\n",
    "                class_name = class_names[cls]\n",
    "                \n",
    "                print(f\"   âœ… Detected: {class_name} ({conf*100:.1f}% confidence)\")\n",
    "                \n",
    "                # Display image with detection\n",
    "                img = result.plot()\n",
    "                cv2.imshow(f'Test Result - {img_path.name}', img)\n",
    "                cv2.waitKey(1000)  # Show for 1 second\n",
    "            else:\n",
    "                print(f\"   âŒ No detections above {confidence_threshold} confidence\")\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def visualize_predictions(model, test_images_path, class_names, num_samples=6):\n",
    "    \n",
    "    print(\"\\nğŸ“Š Creating prediction visualization...\")\n",
    "    \n",
    "    test_path = Path(test_images_path)\n",
    "    image_files = list(test_path.glob(\"*.jpg\"))[:num_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, img_path in enumerate(image_files):\n",
    "        # Run inference\n",
    "        results = model(str(img_path), verbose=False)\n",
    "        \n",
    "        # Get annotated image\n",
    "        img = results[0].plot()\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Display\n",
    "        axes[idx].imshow(img_rgb)\n",
    "        axes[idx].axis('off')\n",
    "        axes[idx].set_title(f'{img_path.name}', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_samples.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"âœ… Visualization saved as 'prediction_samples.png'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fcffd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================\n",
    "# STEP 5: SAVE MODEL\n",
    "# ============================================\n",
    "\n",
    "def save_model(model, save_path=\"xray_threat_detector.pt\"):\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Saving model to: {save_path}\")\n",
    "    \n",
    "    # Export model\n",
    "    model.export(format='torchscript', save_dir=Path(save_path).parent)\n",
    "    \n",
    "    # Also save as .pt\n",
    "    shutil.copy(\n",
    "        model.trainer.best,  # Best weights from training\n",
    "        save_path\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Model saved!\")\n",
    "    print(f\"   Location: {Path(save_path).absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2a178d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ” DATASET DIAGNOSTIC\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Checking path: c:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\Baggages\n",
      "   Path exists: True\n",
      "   Is directory: True\n",
      "\n",
      "ğŸ“‚ Contents of dataset folder:\n",
      "   ğŸ“ Blade/\n",
      "      .jpg:  0\n",
      "      .JPG:  0\n",
      "      .png:  100\n",
      "      .jpeg: 0\n",
      "      TOTAL: 100 images\n",
      "   ğŸ“ Gun/\n",
      "      .jpg:  0\n",
      "      .JPG:  0\n",
      "      .png:  100\n",
      "      .jpeg: 0\n",
      "      TOTAL: 100 images\n",
      "   ğŸ“ Knife/\n",
      "      .jpg:  0\n",
      "      .JPG:  0\n",
      "      .png:  100\n",
      "      .jpeg: 0\n",
      "      TOTAL: 100 images\n",
      "   ğŸ“ Pin/\n",
      "      .jpg:  0\n",
      "      .JPG:  0\n",
      "      .png:  100\n",
      "      .jpeg: 0\n",
      "      TOTAL: 100 images\n",
      "   ğŸ“ Projectile/\n",
      "      .jpg:  0\n",
      "      .JPG:  0\n",
      "      .png:  100\n",
      "      .jpeg: 0\n",
      "      TOTAL: 100 images\n",
      "   ğŸ“ Screw/\n",
      "      .jpg:  0\n",
      "      .JPG:  0\n",
      "      .png:  100\n",
      "      .jpeg: 0\n",
      "      TOTAL: 100 images\n",
      "   ğŸ“ Spring/\n",
      "      .jpg:  0\n",
      "      .JPG:  0\n",
      "      .png:  100\n",
      "      .jpeg: 0\n",
      "      TOTAL: 100 images\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# COMPLETE PIPELINE\n",
    "# ============================================\n",
    "\n",
    "def run_complete_pipeline(\n",
    "    dataset_path,\n",
    "    epochs=50,\n",
    "    img_size=640,\n",
    "    batch_size=16,\n",
    "    model_variant=\"yolov8m.pt\"\n",
    "):\n",
    "    \n",
    "    print(\"\"\"\n",
    "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "    â•‘                                                          â•‘\n",
    "    â•‘         X-RAY THREAT DETECTION - PHASE 1                 â•‘\n",
    "    â•‘              YOLOv8 Training Pipeline                    â•‘\n",
    "    â•‘                                                          â•‘\n",
    "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"\"\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Prepare dataset\n",
    "    class_names, data_yaml_path = prepare_yolo_dataset(dataset_path)\n",
    "    \n",
    "    # Step 2: Train model\n",
    "    model, training_results = train_yolo_model(\n",
    "        data_yaml_path,\n",
    "        epochs=epochs,\n",
    "        img_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        model_name=model_variant\n",
    "    )\n",
    "    \n",
    "    # Step 3: Evaluate model\n",
    "    metrics = evaluate_model(model, data_yaml_path, img_size=img_size)\n",
    "    \n",
    "    # Step 4: Test on sample images\n",
    "    val_images_path = Path(\"xray_yolo_dataset/images/val\")\n",
    "    test_on_images(model, val_images_path, class_names)\n",
    "    visualize_predictions(model, val_images_path, class_names)\n",
    "    \n",
    "    # Step 5: Save model\n",
    "    save_model(model, \"xray_threat_detector_best.pt\")\n",
    "    \n",
    "    # Summary\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nâ±ï¸  Total Time: {elapsed_time/60:.1f} minutes\")\n",
    "    print(f\"\\nğŸ“Š Final Metrics:\")\n",
    "    print(f\"   Classes: {class_names}\")\n",
    "    print(f\"   Training Images: Shown above\")\n",
    "    print(f\"   Validation mAP@50: {metrics.box.map50:.4f}\")\n",
    "    print(f\"   Model saved: xray_threat_detector_best.pt\")\n",
    "    \n",
    "    print(\"\\nğŸ“ Output Files:\")\n",
    "    print(\"   - xray_yolo_dataset/          (Formatted dataset)\")\n",
    "    print(\"   - xray_runs/                  (Training logs)\")\n",
    "    print(\"   - xray_threat_detector_best.pt (Trained model)\")\n",
    "    print(\"   - prediction_samples.png      (Sample predictions)\")\n",
    "    \n",
    "    print(\"\\nğŸš€ Ready for Phase 2: Video Processing!\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Update this to your actual path\n",
    "DATASET_PATH = \"Baggages\"\n",
    "\n",
    "dataset_path = Path(DATASET_PATH)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ” DATASET DIAGNOSTIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nğŸ“ Checking path: {dataset_path.absolute()}\")\n",
    "print(f\"   Path exists: {dataset_path.exists()}\")\n",
    "print(f\"   Is directory: {dataset_path.is_dir()}\")\n",
    "\n",
    "if dataset_path.exists():\n",
    "    print(\"\\nğŸ“‚ Contents of dataset folder:\")\n",
    "    \n",
    "    for item in dataset_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            # Count images in this folder\n",
    "            jpg_count = len(list(item.glob(\"*.jpg\")))\n",
    "            png_count = len(list(item.glob(\"*.png\")))\n",
    "            jpeg_count = len(list(item.glob(\"*.jpeg\")))\n",
    "            JPG_count = len(list(item.glob(\"*.JPG\")))  # Uppercase\n",
    "            \n",
    "            total = jpg_count + png_count + jpeg_count + JPG_count\n",
    "            \n",
    "            print(f\"   ğŸ“ {item.name}/\")\n",
    "            print(f\"      .jpg:  {jpg_count}\")\n",
    "            print(f\"      .JPG:  {JPG_count}\")\n",
    "            print(f\"      .png:  {png_count}\")\n",
    "            print(f\"      .jpeg: {jpeg_count}\")\n",
    "            print(f\"      TOTAL: {total} images\")\n",
    "        else:\n",
    "            print(f\"   ğŸ“„ {item.name} (file, not folder)\")\n",
    "else:\n",
    "    print(\"\\nâŒ Path does NOT exist!\")\n",
    "    print(\"ğŸ’¡ Make sure you updated DATASET_PATH correctly\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84ff94c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "    â•‘                                                          â•‘\n",
      "    â•‘         X-RAY THREAT DETECTION - PHASE 1                 â•‘\n",
      "    â•‘              YOLOv8 Training Pipeline                    â•‘\n",
      "    â•‘                                                          â•‘\n",
      "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "    \n",
      "============================================================\n",
      "ğŸ“¦ PREPARING YOLO DATASET\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Dataset path: c:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\Baggages\n",
      "\n",
      "ğŸ“‹ Classes found: ['Blade', 'Gun', 'Knife', 'Pin', 'Projectile', 'Screw', 'Spring']\n",
      "   Blade: 200 images\n",
      "   Gun: 200 images\n",
      "   Knife: 200 images\n",
      "   Pin: 200 images\n",
      "   Projectile: 200 images\n",
      "   Screw: 200 images\n",
      "   Spring: 200 images\n",
      "\n",
      "âœ… Total images found: 1400\n",
      "\n",
      "âœ… Split complete:\n",
      "   Training: 1120 images\n",
      "   Validation: 280 images\n",
      "\n",
      "ğŸ“ Copying files and creating labels...\n",
      "âœ… Files copied and labels created!\n",
      "\n",
      "âœ… data.yaml created at: xray_yolo_dataset\\data.yaml\n",
      "\n",
      "============================================================\n",
      "ğŸš€ STARTING YOLO TRAINING\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Training Configuration:\n",
      "   Model: yolov8n.pt\n",
      "   Epochs: 10\n",
      "   Image Size: 416\n",
      "   Batch Size: 8\n",
      "\n",
      "ğŸ“¦ Loading pre-trained yolov8n.pt...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 22.2MB/s 0.3s.2s<0.1s\n",
      "\n",
      "ğŸ‹ï¸ Training started...\n",
      "\n",
      "New https://pypi.org/project/ultralytics/8.3.230 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.227  Python-3.10.5 torch-2.4.1+cu118 CPU (Intel Core i5-8265U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=xray_yolo_dataset\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=xray_detector, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=xray_runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_runs\\xray_detector, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=7\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752677  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,012,213 parameters, 3,012,197 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 21.131.0 MB/s, size: 319.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_yolo_dataset\\labels\\train... 672 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 672/672 283.0it/s 2.4s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_yolo_dataset\\labels\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 14.614.8 MB/s, size: 142.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_yolo_dataset\\labels\\val... 252 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 252/252 316.4it/s 0.8s0.3s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_yolo_dataset\\labels\\val.cache\n",
      "Plotting labels to C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_runs\\xray_detector\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_runs\\xray_detector\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10         0G     0.3488      2.863      1.087          8        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0.5it/s 2:482.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 16/16 0.6it/s 27.2s1.4s\n",
      "                   all        252        252      0.539       0.65       0.67       0.64\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10         0G     0.2149      1.436     0.9455          8        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0.5it/s 2:461.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 16/16 0.6it/s 26.4s1.4s\n",
      "                   all        252        252      0.832      0.877      0.965      0.952\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10         0G     0.1932       1.14     0.9334          8        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0.5it/s 2:391.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 16/16 0.6it/s 26.0s1.4s\n",
      "                   all        252        252      0.788      0.804      0.908      0.884\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10         0G     0.1764     0.9548     0.9289          8        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0.3it/s 4:042.5ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 16/16 0.4it/s 37.4s1.8ss\n",
      "                   all        252        252      0.909      0.936      0.991      0.977\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10         0G     0.1536     0.7453     0.9322          8        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0.4it/s 3:083.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 16/16 0.5it/s 32.8s1.6ss\n",
      "                   all        252        252      0.946      0.956       0.98      0.975\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10         0G     0.1448       0.69     0.9168          8        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0.4it/s 3:112.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 16/16 0.6it/s 28.2s1.5ss\n",
      "                   all        252        252      0.948       0.96      0.995      0.994\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10         0G     0.1217     0.6026     0.9073          8        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0.5it/s 3:032.5ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 16/16 0.5it/s 33.7s2.0ss\n",
      "                   all        252        252       0.99      0.989      0.994      0.991\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10         0G     0.1108     0.5672     0.8947          8        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0.5it/s 2:592.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 16/16 0.5it/s 32.2s1.5ss\n",
      "                   all        252        252      0.993      0.992      0.994      0.993\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10         0G    0.09071     0.4798     0.9011          8        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0.5it/s 2:592.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 16/16 0.5it/s 32.4s1.7ss\n",
      "                   all        252        252      0.996      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10         0G    0.07447     0.4338     0.9008          8        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0.4it/s 3:072.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 16/16 0.6it/s 28.5s1.5ss\n",
      "                   all        252        252      0.998          1      0.995      0.995\n",
      "\n",
      "10 epochs completed in 0.599 hours.\n",
      "Optimizer stripped from C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_runs\\xray_detector\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_runs\\xray_detector\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_runs\\xray_detector\\weights\\best.pt...\n",
      "Ultralytics 8.3.227  Python-3.10.5 torch-2.4.1+cu118 CPU (Intel Core i5-8265U 1.60GHz)\n",
      "Model summary (fused): 72 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 16/16 0.6it/s 26.6s1.4s\n",
      "                   all        252        252      0.998          1      0.995      0.995\n",
      "                 Blade         35         35      0.998          1      0.995      0.995\n",
      "                   Gun         38         38      0.998          1      0.995      0.995\n",
      "                 Knife         38         38      0.998          1      0.995      0.995\n",
      "                   Pin         37         37      0.998          1      0.995      0.995\n",
      "            Projectile         35         35      0.998          1      0.995      0.995\n",
      "                 Screw         37         37      0.998          1      0.995      0.995\n",
      "                Spring         32         32      0.999          1      0.995      0.995\n",
      "Speed: 1.3ms preprocess, 63.9ms inference, 0.0ms loss, 5.7ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_runs\\xray_detector\u001b[0m\n",
      "\n",
      "============================================================\n",
      "âœ… TRAINING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š EVALUATING MODEL\n",
      "============================================================\n",
      "Ultralytics 8.3.227  Python-3.10.5 torch-2.4.1+cu118 CPU (Intel Core i5-8265U 1.60GHz)\n",
      "Model summary (fused): 72 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 178.6374.0 MB/s, size: 304.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_yolo_dataset\\labels\\val.cache... 252 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 252/252  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 16/16 0.6it/s 25.9s1.2s\n",
      "                   all        252        252      0.998          1      0.995      0.995\n",
      "                 Blade         35         35      0.998          1      0.995      0.995\n",
      "                   Gun         38         38      0.998          1      0.995      0.995\n",
      "                 Knife         38         38      0.998          1      0.995      0.995\n",
      "                   Pin         37         37      0.998          1      0.995      0.995\n",
      "            Projectile         35         35      0.998          1      0.995      0.995\n",
      "                 Screw         37         37      0.998          1      0.995      0.995\n",
      "                Spring         32         32      0.999          1      0.995      0.995\n",
      "Speed: 1.0ms preprocess, 59.2ms inference, 0.0ms loss, 5.1ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\runs\\detect\\val\u001b[0m\n",
      "\n",
      "ğŸ“ˆ Validation Results:\n",
      "   Precision: 0.9981\n",
      "   Recall: 1.0000\n",
      "   mAP@50: 0.9950\n",
      "   mAP@50-95: 0.9950\n",
      "\n",
      "============================================================\n",
      "ğŸ§ª TESTING ON SAMPLE IMAGES\n",
      "============================================================\n",
      "\n",
      "ğŸ“¸ Found 252 test images\n",
      "\n",
      "1. Testing: Blade_B0001_0002.jpg\n",
      "   âœ… Detected: Blade (99.7% confidence)\n",
      "\n",
      "2. Testing: Blade_B0001_0003.jpg\n",
      "   âœ… Detected: Blade (99.7% confidence)\n",
      "\n",
      "3. Testing: Blade_B0001_0004.jpg\n",
      "   âœ… Detected: Blade (99.7% confidence)\n",
      "\n",
      "4. Testing: Blade_B0001_0007.jpg\n",
      "   âœ… Detected: Blade (99.8% confidence)\n",
      "\n",
      "5. Testing: Blade_B0001_0008.jpg\n",
      "   âœ… Detected: Blade (99.7% confidence)\n",
      "\n",
      "6. Testing: Blade_B0007_0005.jpg\n",
      "   âœ… Detected: Blade (99.3% confidence)\n",
      "\n",
      "7. Testing: Blade_B0007_0007.jpg\n",
      "   âœ… Detected: Blade (99.2% confidence)\n",
      "\n",
      "8. Testing: Blade_B0051_0013.jpg\n",
      "   âœ… Detected: Blade (99.4% confidence)\n",
      "\n",
      "9. Testing: Blade_B0051_0014.jpg\n",
      "   âœ… Detected: Blade (99.5% confidence)\n",
      "\n",
      "10. Testing: Blade_B0056_0344.jpg\n",
      "   âœ… Detected: Blade (99.0% confidence)\n",
      "\n",
      "ğŸ“Š Creating prediction visualization...\n",
      "âœ… Visualization saved as 'prediction_samples.png'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Saving model to: xray_threat_detector_best.pt\n",
      "Ultralytics 8.3.227  Python-3.10.5 torch-2.4.1+cu118 CPU (Intel Core i5-8265U 1.60GHz)\n",
      " ProTip: Export to OpenVINO format for best performance on Intel hardware. Learn more at https://docs.ultralytics.com/integrations/openvino/\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_runs\\xray_detector\\weights\\best.pt' with input shape (1, 3, 416, 416) BCHW and output shape(s) (1, 11, 3549) (5.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.4.1+cu118...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success  4.6s, saved as 'C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_runs\\xray_detector\\weights\\best.torchscript' (11.8 MB)\n",
      "\n",
      "Export complete (4.9s)\n",
      "Results saved to \u001b[1mC:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_runs\\xray_detector\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_runs\\xray_detector\\weights\\best.torchscript imgsz=416  \n",
      "Validate:        yolo val task=detect model=C:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_runs\\xray_detector\\weights\\best.torchscript imgsz=416 data=xray_yolo_dataset\\data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "âœ… Model saved!\n",
      "   Location: c:\\Users\\Enzo\\Desktop\\CODING PROJECTS\\Python Exercises\\Mini_Rapiscan920CT\\xray_threat_detector_best.pt\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ PIPELINE COMPLETE!\n",
      "============================================================\n",
      "\n",
      "â±ï¸  Total Time: 38.0 minutes\n",
      "\n",
      "ğŸ“Š Final Metrics:\n",
      "   Classes: ['Blade', 'Gun', 'Knife', 'Pin', 'Projectile', 'Screw', 'Spring']\n",
      "   Training Images: Shown above\n",
      "   Validation mAP@50: 0.9950\n",
      "   Model saved: xray_threat_detector_best.pt\n",
      "\n",
      "ğŸ“ Output Files:\n",
      "   - xray_yolo_dataset/          (Formatted dataset)\n",
      "   - xray_runs/                  (Training logs)\n",
      "   - xray_threat_detector_best.pt (Trained model)\n",
      "   - prediction_samples.png      (Sample predictions)\n",
      "\n",
      "ğŸš€ Ready for Phase 2: Video Processing!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Update this path to your dataset location\n",
    "    DATASET_PATH = \"Baggages\"\n",
    "    \n",
    "    # Run everything\n",
    "    run_complete_pipeline(\n",
    "        dataset_path= DATASET_PATH,  # Small dataset!\n",
    "        epochs=10,\n",
    "        img_size=416,\n",
    "        batch_size=8,\n",
    "    model_variant=\"yolov8n.pt\"\n",
    "    )\n",
    "\n",
    "# Should complete in 2-3 hours. THE ENTIRE PROCESS, NOT JUST AN EPOCH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
